{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44902\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nn.layers.transformer import Transformer\n",
    "from nn.losses.classification import CategoricalCrossentropy\n",
    "from nn.optimizers.first_order import Adam\n",
    "\n",
    "with open(\"../data/resources/saved_datasets/tiny_shakespeare.txt\", \"r\") as f:\n",
    "    o = f.read().splitlines()\n",
    "\n",
    "datastr = \" \".join(o).lower()#[:100]\n",
    "split = int(0.8 * len(datastr))\n",
    "train = datastr[:split]\n",
    "val   = datastr[split:]\n",
    "#datastr = \"abcd\"\n",
    "#train = 1000*\"abc\"\n",
    "#val = 1000*\"bcd\"\n",
    "vocab = sorted(list(set(datastr)))\n",
    "\n",
    "def sample_data(example_len, data):\n",
    "    start_idx = np.random.randint(len(data)-example_len)\n",
    "    example = data[start_idx:start_idx+example_len]\n",
    "    return np.array([vocab.index(c) for c in example])\n",
    "\n",
    "def generate_batch(batch_size, block_size, data):\n",
    "    data = np.array([sample_data(block_size+1, data) for _ in range(batch_size)])\n",
    "    decoder_inputs = data[:, :-1]\n",
    "    targets        = data[:, 1:]\n",
    "    encoder_inputs = np.ones_like(decoder_inputs)\n",
    "    return encoder_inputs, decoder_inputs, targets\n",
    "\n",
    "block_size = 32                 # sequence length\n",
    "batch_size = 128                # batch size\n",
    "d_model = 32                    # dimesion of embedding\n",
    "d_ff = d_model*2                # FF projection dimension\n",
    "n_heads = 8                     # number of attention heads\n",
    "d_hidden = d_model // n_heads   # attention dimension\n",
    "n_layers = 2                    # number of encoder and decoder layers\n",
    "vocab_size = len(vocab)         # size of vocabulary\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "model = Transformer(vocab_size, n_layers, n_heads, d_model, d_hidden, d_ff)\n",
    "loss  = CategoricalCrossentropy()\n",
    "optim = Adam(model.parameters(), 1e-2, 0.9, 0.999)\n",
    "history = {\"train\": [], \"val\": []}\n",
    "\n",
    "print(model.n_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 8.054, val loss: 7.252\n",
      "step: 10, train loss: 5.576, val loss: 4.188\n",
      "step: 20, train loss: 4.017, val loss: 3.749\n",
      "step: 30, train loss: 3.607, val loss: 3.491\n",
      "step: 40, train loss: 3.409, val loss: 3.326\n",
      "step: 50, train loss: 3.265, val loss: 3.212\n",
      "step: 60, train loss: 3.183, val loss: 3.140\n",
      "step: 70, train loss: 3.114, val loss: 3.098\n",
      "step: 80, train loss: 3.072, val loss: 3.047\n",
      "step: 90, train loss: 3.039, val loss: 3.020\n",
      "step: 100, train loss: 3.008, val loss: 2.997\n",
      "step: 110, train loss: 2.994, val loss: 2.984\n",
      "step: 120, train loss: 2.988, val loss: 2.985\n",
      "step: 130, train loss: 2.979, val loss: 2.984\n",
      "step: 140, train loss: 2.993, val loss: 2.975\n",
      "step: 150, train loss: 2.975, val loss: 2.977\n",
      "step: 160, train loss: 2.977, val loss: 2.969\n",
      "step: 170, train loss: 2.978, val loss: 2.973\n",
      "step: 180, train loss: 2.975, val loss: 2.970\n",
      "step: 190, train loss: 2.983, val loss: 2.973\n",
      "step: 200, train loss: 2.982, val loss: 2.976\n",
      "step: 210, train loss: 2.976, val loss: 2.968\n",
      "step: 220, train loss: 2.971, val loss: 2.977\n",
      "step: 230, train loss: 2.976, val loss: 2.976\n",
      "step: 240, train loss: 2.970, val loss: 2.973\n",
      "step: 250, train loss: 2.973, val loss: 2.973\n",
      "step: 260, train loss: 2.971, val loss: 2.968\n",
      "step: 270, train loss: 2.971, val loss: 2.967\n",
      "step: 280, train loss: 2.975, val loss: 2.975\n",
      "step: 290, train loss: 2.975, val loss: 2.966\n",
      "step: 300, train loss: 2.968, val loss: 2.965\n",
      "step: 310, train loss: 2.970, val loss: 2.968\n",
      "step: 320, train loss: 2.979, val loss: 2.966\n",
      "step: 330, train loss: 2.970, val loss: 2.967\n",
      "step: 340, train loss: 2.970, val loss: 2.968\n",
      "step: 350, train loss: 2.971, val loss: 2.966\n",
      "step: 360, train loss: 2.967, val loss: 2.969\n",
      "step: 370, train loss: 2.973, val loss: 2.967\n",
      "step: 380, train loss: 2.966, val loss: 2.966\n",
      "step: 390, train loss: 2.977, val loss: 2.963\n",
      "step: 400, train loss: 2.968, val loss: 2.963\n",
      "step: 410, train loss: 2.966, val loss: 2.969\n",
      "step: 420, train loss: 2.967, val loss: 2.965\n",
      "step: 430, train loss: 2.969, val loss: 2.962\n",
      "step: 440, train loss: 2.965, val loss: 2.973\n",
      "step: 450, train loss: 2.968, val loss: 2.967\n",
      "step: 460, train loss: 2.965, val loss: 2.967\n",
      "step: 470, train loss: 2.965, val loss: 2.967\n",
      "step: 480, train loss: 2.972, val loss: 2.961\n",
      "step: 490, train loss: 2.965, val loss: 2.960\n",
      "step: 500, train loss: 2.968, val loss: 2.966\n",
      "step: 510, train loss: 2.957, val loss: 2.964\n",
      "step: 520, train loss: 2.960, val loss: 2.964\n",
      "step: 530, train loss: 2.959, val loss: 2.963\n",
      "step: 540, train loss: 2.963, val loss: 2.960\n",
      "step: 550, train loss: 2.963, val loss: 2.966\n",
      "step: 560, train loss: 2.953, val loss: 2.958\n",
      "step: 570, train loss: 2.961, val loss: 2.960\n",
      "step: 580, train loss: 2.956, val loss: 2.952\n",
      "step: 590, train loss: 2.958, val loss: 2.957\n",
      "step: 600, train loss: 2.959, val loss: 2.959\n",
      "step: 610, train loss: 2.960, val loss: 2.952\n",
      "step: 620, train loss: 2.954, val loss: 2.956\n",
      "step: 630, train loss: 2.955, val loss: 2.955\n",
      "step: 640, train loss: 2.956, val loss: 2.950\n",
      "step: 650, train loss: 2.957, val loss: 2.951\n",
      "step: 660, train loss: 2.949, val loss: 2.955\n",
      "step: 670, train loss: 2.955, val loss: 2.949\n",
      "step: 680, train loss: 2.955, val loss: 2.948\n",
      "step: 690, train loss: 2.955, val loss: 2.954\n",
      "step: 700, train loss: 2.954, val loss: 2.943\n",
      "step: 710, train loss: 2.950, val loss: 2.949\n",
      "step: 720, train loss: 2.949, val loss: 2.942\n",
      "step: 730, train loss: 2.952, val loss: 2.944\n",
      "step: 740, train loss: 2.941, val loss: 2.941\n",
      "step: 750, train loss: 2.940, val loss: 2.941\n",
      "step: 760, train loss: 2.940, val loss: 2.945\n",
      "step: 770, train loss: 2.943, val loss: 2.941\n",
      "step: 780, train loss: 2.940, val loss: 2.935\n",
      "step: 790, train loss: 2.944, val loss: 2.939\n",
      "step: 800, train loss: 2.938, val loss: 2.936\n",
      "step: 810, train loss: 2.945, val loss: 2.938\n",
      "step: 820, train loss: 2.938, val loss: 2.934\n",
      "step: 830, train loss: 2.939, val loss: 2.934\n",
      "step: 840, train loss: 2.936, val loss: 2.924\n",
      "step: 850, train loss: 2.936, val loss: 2.938\n",
      "step: 860, train loss: 2.931, val loss: 2.930\n",
      "step: 870, train loss: 2.934, val loss: 2.928\n",
      "step: 880, train loss: 2.934, val loss: 2.934\n",
      "step: 890, train loss: 2.929, val loss: 2.932\n",
      "step: 900, train loss: 2.931, val loss: 2.922\n",
      "step: 910, train loss: 2.928, val loss: 2.927\n",
      "step: 920, train loss: 2.927, val loss: 2.926\n",
      "step: 930, train loss: 2.926, val loss: 2.922\n",
      "step: 940, train loss: 2.932, val loss: 2.932\n",
      "step: 950, train loss: 2.917, val loss: 2.921\n",
      "step: 960, train loss: 2.925, val loss: 2.927\n",
      "step: 970, train loss: 2.910, val loss: 2.912\n",
      "step: 980, train loss: 2.926, val loss: 2.923\n",
      "step: 990, train loss: 2.921, val loss: 2.921\n",
      "step: 1000, train loss: 2.921, val loss: 2.916\n",
      "step: 1010, train loss: 2.913, val loss: 2.917\n",
      "step: 1020, train loss: 2.919, val loss: 2.917\n",
      "step: 1030, train loss: 2.918, val loss: 2.914\n",
      "step: 1040, train loss: 2.908, val loss: 2.916\n",
      "step: 1050, train loss: 2.923, val loss: 2.912\n",
      "step: 1060, train loss: 2.914, val loss: 2.912\n",
      "step: 1070, train loss: 2.922, val loss: 2.909\n",
      "step: 1080, train loss: 2.908, val loss: 2.911\n",
      "step: 1090, train loss: 2.916, val loss: 2.912\n",
      "step: 1100, train loss: 2.913, val loss: 2.915\n",
      "step: 1110, train loss: 2.911, val loss: 2.911\n",
      "step: 1120, train loss: 2.912, val loss: 2.911\n",
      "step: 1130, train loss: 2.905, val loss: 2.905\n",
      "step: 1140, train loss: 2.900, val loss: 2.907\n",
      "step: 1150, train loss: 2.910, val loss: 2.907\n",
      "step: 1160, train loss: 2.906, val loss: 2.901\n",
      "step: 1170, train loss: 2.907, val loss: 2.903\n",
      "step: 1180, train loss: 2.902, val loss: 2.903\n",
      "step: 1190, train loss: 2.902, val loss: 2.901\n",
      "step: 1200, train loss: 2.903, val loss: 2.902\n",
      "step: 1210, train loss: 2.900, val loss: 2.904\n",
      "step: 1220, train loss: 2.908, val loss: 2.904\n",
      "step: 1230, train loss: 2.903, val loss: 2.898\n",
      "step: 1240, train loss: 2.905, val loss: 2.899\n",
      "step: 1250, train loss: 2.896, val loss: 2.899\n",
      "step: 1260, train loss: 2.896, val loss: 2.893\n",
      "step: 1270, train loss: 2.892, val loss: 2.891\n",
      "step: 1280, train loss: 2.896, val loss: 2.888\n",
      "step: 1290, train loss: 2.892, val loss: 2.894\n",
      "step: 1300, train loss: 2.892, val loss: 2.891\n",
      "step: 1310, train loss: 2.896, val loss: 2.886\n",
      "step: 1320, train loss: 2.892, val loss: 2.887\n",
      "step: 1330, train loss: 2.889, val loss: 2.887\n",
      "step: 1340, train loss: 2.894, val loss: 2.891\n",
      "step: 1350, train loss: 2.892, val loss: 2.891\n",
      "step: 1360, train loss: 2.882, val loss: 2.886\n",
      "step: 1370, train loss: 2.886, val loss: 2.889\n",
      "step: 1380, train loss: 2.887, val loss: 2.873\n",
      "step: 1390, train loss: 2.888, val loss: 2.882\n",
      "step: 1400, train loss: 2.880, val loss: 2.885\n",
      "step: 1410, train loss: 2.877, val loss: 2.885\n",
      "step: 1420, train loss: 2.872, val loss: 2.881\n",
      "step: 1430, train loss: 2.878, val loss: 2.871\n",
      "step: 1440, train loss: 2.883, val loss: 2.882\n",
      "step: 1450, train loss: 2.874, val loss: 2.873\n",
      "step: 1460, train loss: 2.878, val loss: 2.879\n",
      "step: 1470, train loss: 2.877, val loss: 2.876\n",
      "step: 1480, train loss: 2.877, val loss: 2.874\n",
      "step: 1490, train loss: 2.873, val loss: 2.868\n",
      "step: 1500, train loss: 2.866, val loss: 2.873\n",
      "step: 1510, train loss: 2.875, val loss: 2.871\n",
      "step: 1520, train loss: 2.878, val loss: 2.862\n",
      "step: 1530, train loss: 2.869, val loss: 2.874\n",
      "step: 1540, train loss: 2.871, val loss: 2.862\n",
      "step: 1550, train loss: 2.861, val loss: 2.870\n",
      "step: 1560, train loss: 2.866, val loss: 2.866\n",
      "step: 1570, train loss: 2.867, val loss: 2.866\n",
      "step: 1580, train loss: 2.871, val loss: 2.861\n",
      "step: 1590, train loss: 2.871, val loss: 2.864\n",
      "step: 1600, train loss: 2.863, val loss: 2.864\n",
      "step: 1610, train loss: 2.863, val loss: 2.866\n",
      "step: 1620, train loss: 2.867, val loss: 2.859\n",
      "step: 1630, train loss: 2.866, val loss: 2.856\n",
      "step: 1640, train loss: 2.855, val loss: 2.860\n",
      "step: 1650, train loss: 2.862, val loss: 2.859\n",
      "step: 1660, train loss: 2.865, val loss: 2.853\n",
      "step: 1670, train loss: 2.859, val loss: 2.859\n",
      "step: 1680, train loss: 2.858, val loss: 2.853\n",
      "step: 1690, train loss: 2.864, val loss: 2.855\n",
      "step: 1700, train loss: 2.856, val loss: 2.857\n",
      "step: 1710, train loss: 2.847, val loss: 2.859\n",
      "step: 1720, train loss: 2.853, val loss: 2.851\n",
      "step: 1730, train loss: 2.850, val loss: 2.853\n",
      "step: 1740, train loss: 2.853, val loss: 2.853\n",
      "step: 1750, train loss: 2.855, val loss: 2.850\n",
      "step: 1760, train loss: 2.855, val loss: 2.846\n",
      "step: 1770, train loss: 2.856, val loss: 2.853\n",
      "step: 1780, train loss: 2.857, val loss: 2.850\n",
      "step: 1790, train loss: 2.860, val loss: 2.847\n",
      "step: 1800, train loss: 2.856, val loss: 2.850\n",
      "step: 1810, train loss: 2.855, val loss: 2.851\n",
      "step: 1820, train loss: 2.841, val loss: 2.849\n",
      "step: 1830, train loss: 2.846, val loss: 2.851\n",
      "step: 1840, train loss: 2.850, val loss: 2.844\n",
      "step: 1850, train loss: 2.848, val loss: 2.844\n",
      "step: 1860, train loss: 2.854, val loss: 2.845\n",
      "step: 1870, train loss: 2.853, val loss: 2.842\n",
      "step: 1880, train loss: 2.841, val loss: 2.845\n",
      "step: 1890, train loss: 2.847, val loss: 2.851\n",
      "step: 1900, train loss: 2.851, val loss: 2.841\n",
      "step: 1910, train loss: 2.850, val loss: 2.844\n",
      "step: 1920, train loss: 2.850, val loss: 2.845\n",
      "step: 1930, train loss: 2.842, val loss: 2.842\n",
      "step: 1940, train loss: 2.849, val loss: 2.842\n",
      "step: 1950, train loss: 2.845, val loss: 2.838\n",
      "step: 1960, train loss: 2.854, val loss: 2.839\n",
      "step: 1970, train loss: 2.852, val loss: 2.842\n",
      "step: 1980, train loss: 2.843, val loss: 2.846\n",
      "step: 1990, train loss: 2.839, val loss: 2.844\n",
      "step: 2000, train loss: 2.842, val loss: 2.843\n",
      "step: 2010, train loss: 2.846, val loss: 2.843\n",
      "step: 2020, train loss: 2.837, val loss: 2.836\n",
      "step: 2030, train loss: 2.838, val loss: 2.832\n",
      "step: 2040, train loss: 2.835, val loss: 2.833\n",
      "step: 2050, train loss: 2.842, val loss: 2.837\n",
      "step: 2060, train loss: 2.838, val loss: 2.835\n",
      "step: 2070, train loss: 2.842, val loss: 2.835\n",
      "step: 2080, train loss: 2.842, val loss: 2.835\n",
      "step: 2090, train loss: 2.833, val loss: 2.831\n",
      "step: 2100, train loss: 2.826, val loss: 2.832\n",
      "step: 2110, train loss: 2.841, val loss: 2.836\n",
      "step: 2120, train loss: 2.839, val loss: 2.834\n",
      "step: 2130, train loss: 2.835, val loss: 2.836\n",
      "step: 2140, train loss: 2.842, val loss: 2.835\n",
      "step: 2150, train loss: 2.836, val loss: 2.833\n",
      "step: 2160, train loss: 2.838, val loss: 2.832\n",
      "step: 2170, train loss: 2.829, val loss: 2.830\n",
      "step: 2180, train loss: 2.838, val loss: 2.828\n",
      "step: 2190, train loss: 2.834, val loss: 2.832\n",
      "step: 2200, train loss: 2.836, val loss: 2.831\n",
      "step: 2210, train loss: 2.834, val loss: 2.832\n",
      "step: 2220, train loss: 2.826, val loss: 2.834\n",
      "step: 2230, train loss: 2.829, val loss: 2.826\n",
      "step: 2240, train loss: 2.826, val loss: 2.825\n",
      "step: 2250, train loss: 2.829, val loss: 2.829\n",
      "step: 2260, train loss: 2.826, val loss: 2.822\n",
      "step: 2270, train loss: 2.829, val loss: 2.823\n",
      "step: 2280, train loss: 2.824, val loss: 2.824\n",
      "step: 2290, train loss: 2.822, val loss: 2.828\n",
      "step: 2300, train loss: 2.829, val loss: 2.822\n",
      "step: 2310, train loss: 2.825, val loss: 2.820\n",
      "step: 2320, train loss: 2.823, val loss: 2.832\n",
      "step: 2330, train loss: 2.827, val loss: 2.822\n",
      "step: 2340, train loss: 2.818, val loss: 2.815\n",
      "step: 2350, train loss: 2.823, val loss: 2.818\n",
      "step: 2360, train loss: 2.826, val loss: 2.817\n",
      "step: 2370, train loss: 2.826, val loss: 2.818\n",
      "step: 2380, train loss: 2.825, val loss: 2.810\n",
      "step: 2390, train loss: 2.814, val loss: 2.815\n",
      "step: 2400, train loss: 2.811, val loss: 2.796\n",
      "step: 2410, train loss: 2.802, val loss: 2.798\n",
      "step: 2420, train loss: 2.792, val loss: 2.789\n",
      "step: 2430, train loss: 2.779, val loss: 2.770\n",
      "step: 2440, train loss: 2.769, val loss: 2.762\n",
      "step: 2450, train loss: 2.748, val loss: 2.735\n",
      "step: 2460, train loss: 2.726, val loss: 2.720\n",
      "step: 2470, train loss: 2.711, val loss: 2.683\n",
      "step: 2480, train loss: 2.668, val loss: 2.667\n",
      "step: 2490, train loss: 2.650, val loss: 2.625\n",
      "step: 2500, train loss: 2.607, val loss: 2.594\n",
      "step: 2510, train loss: 2.575, val loss: 2.536\n",
      "step: 2520, train loss: 2.523, val loss: 2.511\n",
      "step: 2530, train loss: 2.471, val loss: 2.438\n",
      "step: 2540, train loss: 2.422, val loss: 2.371\n",
      "step: 2550, train loss: 2.339, val loss: 2.296\n",
      "step: 2560, train loss: 2.281, val loss: 2.238\n",
      "step: 2570, train loss: 2.197, val loss: 2.148\n",
      "step: 2580, train loss: 2.126, val loss: 2.078\n",
      "step: 2590, train loss: 2.010, val loss: 1.963\n",
      "step: 2600, train loss: 1.961, val loss: 1.905\n",
      "step: 2610, train loss: 1.814, val loss: 1.758\n",
      "step: 2620, train loss: 1.674, val loss: 1.593\n",
      "step: 2630, train loss: 1.551, val loss: 1.534\n",
      "step: 2640, train loss: 1.481, val loss: 1.378\n",
      "step: 2650, train loss: 1.332, val loss: 1.265\n",
      "step: 2660, train loss: 1.216, val loss: 1.139\n",
      "step: 2670, train loss: 1.092, val loss: 1.043\n",
      "step: 2680, train loss: 0.999, val loss: 0.944\n",
      "step: 2690, train loss: 1.000, val loss: 0.974\n",
      "step: 2700, train loss: 0.895, val loss: 0.810\n",
      "step: 2710, train loss: 0.740, val loss: 0.675\n",
      "step: 2720, train loss: 0.621, val loss: 0.560\n",
      "step: 2730, train loss: 0.545, val loss: 0.498\n",
      "step: 2740, train loss: 0.454, val loss: 0.413\n",
      "step: 2750, train loss: 0.387, val loss: 0.358\n",
      "step: 2760, train loss: 0.334, val loss: 0.321\n",
      "step: 2770, train loss: 0.312, val loss: 0.300\n",
      "step: 2780, train loss: 0.290, val loss: 0.284\n",
      "step: 2790, train loss: 0.279, val loss: 0.268\n",
      "step: 2800, train loss: 0.264, val loss: 0.252\n",
      "step: 2810, train loss: 0.236, val loss: 0.227\n",
      "step: 2820, train loss: 0.220, val loss: 0.218\n",
      "step: 2830, train loss: 0.209, val loss: 0.207\n",
      "step: 2840, train loss: 0.199, val loss: 0.196\n",
      "step: 2850, train loss: 0.193, val loss: 0.188\n",
      "step: 2860, train loss: 0.187, val loss: 0.182\n",
      "step: 2870, train loss: 0.182, val loss: 0.181\n",
      "step: 2880, train loss: 0.178, val loss: 0.178\n",
      "step: 2890, train loss: 0.173, val loss: 0.176\n",
      "step: 2900, train loss: 0.170, val loss: 0.170\n",
      "step: 2910, train loss: 0.167, val loss: 0.169\n",
      "step: 2920, train loss: 0.165, val loss: 0.166\n",
      "step: 2930, train loss: 0.162, val loss: 0.159\n",
      "step: 2940, train loss: 0.162, val loss: 0.158\n",
      "step: 2950, train loss: 0.158, val loss: 0.158\n",
      "step: 2960, train loss: 0.155, val loss: 0.157\n",
      "step: 2970, train loss: 0.157, val loss: 0.153\n",
      "step: 2980, train loss: 0.150, val loss: 0.155\n",
      "step: 2990, train loss: 0.150, val loss: 0.149\n",
      "step: 3000, train loss: 0.147, val loss: 0.150\n",
      "step: 3010, train loss: 0.147, val loss: 0.149\n",
      "step: 3020, train loss: 0.144, val loss: 0.145\n",
      "step: 3030, train loss: 0.143, val loss: 0.143\n",
      "step: 3040, train loss: 0.141, val loss: 0.141\n",
      "step: 3050, train loss: 0.140, val loss: 0.140\n",
      "step: 3060, train loss: 0.138, val loss: 0.136\n",
      "step: 3070, train loss: 0.135, val loss: 0.137\n",
      "step: 3080, train loss: 0.135, val loss: 0.137\n",
      "step: 3090, train loss: 0.133, val loss: 0.131\n",
      "step: 3100, train loss: 0.133, val loss: 0.131\n",
      "step: 3110, train loss: 0.129, val loss: 0.131\n",
      "step: 3120, train loss: 0.130, val loss: 0.130\n",
      "step: 3130, train loss: 0.127, val loss: 0.127\n",
      "step: 3140, train loss: 0.125, val loss: 0.127\n",
      "step: 3150, train loss: 0.126, val loss: 0.123\n",
      "step: 3160, train loss: 0.122, val loss: 0.121\n",
      "step: 3170, train loss: 0.121, val loss: 0.120\n",
      "step: 3180, train loss: 0.120, val loss: 0.119\n",
      "step: 3190, train loss: 0.116, val loss: 0.118\n",
      "step: 3200, train loss: 0.118, val loss: 0.117\n",
      "step: 3210, train loss: 0.119, val loss: 0.116\n",
      "step: 3220, train loss: 0.116, val loss: 0.116\n",
      "step: 3230, train loss: 0.115, val loss: 0.113\n",
      "step: 3240, train loss: 0.113, val loss: 0.111\n",
      "step: 3250, train loss: 0.112, val loss: 0.112\n",
      "step: 3260, train loss: 0.111, val loss: 0.111\n",
      "step: 3270, train loss: 0.109, val loss: 0.110\n",
      "step: 3280, train loss: 0.109, val loss: 0.109\n",
      "step: 3290, train loss: 0.110, val loss: 0.108\n",
      "step: 3300, train loss: 0.108, val loss: 0.111\n",
      "step: 3310, train loss: 0.107, val loss: 0.109\n",
      "step: 3320, train loss: 0.108, val loss: 0.106\n",
      "step: 3330, train loss: 0.104, val loss: 0.105\n",
      "step: 3340, train loss: 0.103, val loss: 0.104\n",
      "step: 3350, train loss: 0.105, val loss: 0.104\n",
      "step: 3360, train loss: 0.101, val loss: 0.103\n",
      "step: 3370, train loss: 0.103, val loss: 0.103\n",
      "step: 3380, train loss: 0.102, val loss: 0.103\n",
      "step: 3390, train loss: 0.100, val loss: 0.102\n",
      "step: 3400, train loss: 0.100, val loss: 0.102\n",
      "step: 3410, train loss: 0.100, val loss: 0.099\n",
      "step: 3420, train loss: 0.099, val loss: 0.097\n",
      "step: 3430, train loss: 0.098, val loss: 0.098\n",
      "step: 3440, train loss: 0.099, val loss: 0.099\n",
      "step: 3450, train loss: 0.100, val loss: 0.099\n",
      "step: 3460, train loss: 0.099, val loss: 0.098\n",
      "step: 3470, train loss: 0.098, val loss: 0.098\n",
      "step: 3480, train loss: 0.097, val loss: 0.097\n",
      "step: 3490, train loss: 0.097, val loss: 0.097\n",
      "step: 3500, train loss: 0.095, val loss: 0.097\n",
      "step: 3510, train loss: 0.095, val loss: 0.096\n",
      "step: 3520, train loss: 0.096, val loss: 0.095\n",
      "step: 3530, train loss: 0.093, val loss: 0.094\n",
      "step: 3540, train loss: 0.095, val loss: 0.093\n",
      "step: 3550, train loss: 0.095, val loss: 0.095\n",
      "step: 3560, train loss: 0.094, val loss: 0.095\n",
      "step: 3570, train loss: 0.096, val loss: 0.093\n",
      "step: 3580, train loss: 0.093, val loss: 0.093\n",
      "step: 3590, train loss: 0.093, val loss: 0.093\n",
      "step: 3600, train loss: 0.093, val loss: 0.094\n",
      "step: 3610, train loss: 0.092, val loss: 0.093\n",
      "step: 3620, train loss: 0.093, val loss: 0.094\n",
      "step: 3630, train loss: 0.091, val loss: 0.093\n",
      "step: 3640, train loss: 0.093, val loss: 0.093\n",
      "step: 3650, train loss: 0.092, val loss: 0.092\n",
      "step: 3660, train loss: 0.093, val loss: 0.093\n",
      "step: 3670, train loss: 0.090, val loss: 0.093\n",
      "step: 3680, train loss: 0.093, val loss: 0.091\n",
      "step: 3690, train loss: 0.090, val loss: 0.093\n",
      "step: 3700, train loss: 0.092, val loss: 0.090\n",
      "step: 3710, train loss: 0.091, val loss: 0.091\n",
      "step: 3720, train loss: 0.092, val loss: 0.091\n",
      "step: 3730, train loss: 0.090, val loss: 0.093\n",
      "step: 3740, train loss: 0.092, val loss: 0.091\n",
      "step: 3750, train loss: 0.090, val loss: 0.090\n",
      "step: 3760, train loss: 0.091, val loss: 0.091\n",
      "step: 3770, train loss: 0.091, val loss: 0.092\n",
      "step: 3780, train loss: 0.091, val loss: 0.090\n",
      "step: 3790, train loss: 0.090, val loss: 0.091\n",
      "step: 3800, train loss: 0.089, val loss: 0.092\n",
      "step: 3810, train loss: 0.091, val loss: 0.091\n",
      "step: 3820, train loss: 0.091, val loss: 0.089\n",
      "step: 3830, train loss: 0.089, val loss: 0.090\n",
      "step: 3840, train loss: 0.088, val loss: 0.089\n",
      "step: 3850, train loss: 0.091, val loss: 0.090\n",
      "step: 3860, train loss: 0.089, val loss: 0.090\n",
      "step: 3870, train loss: 0.088, val loss: 0.089\n",
      "step: 3880, train loss: 0.089, val loss: 0.090\n",
      "step: 3890, train loss: 0.090, val loss: 0.092\n",
      "step: 3900, train loss: 0.089, val loss: 0.091\n",
      "step: 3910, train loss: 0.087, val loss: 0.088\n",
      "step: 3920, train loss: 0.089, val loss: 0.091\n",
      "step: 3930, train loss: 0.090, val loss: 0.090\n",
      "step: 3940, train loss: 0.088, val loss: 0.090\n",
      "step: 3950, train loss: 0.088, val loss: 0.088\n",
      "step: 3960, train loss: 0.086, val loss: 0.090\n",
      "step: 3970, train loss: 0.087, val loss: 0.089\n",
      "step: 3980, train loss: 0.088, val loss: 0.089\n",
      "step: 3990, train loss: 0.089, val loss: 0.089\n",
      "step: 4000, train loss: 0.087, val loss: 0.086\n",
      "step: 4010, train loss: 0.088, val loss: 0.088\n",
      "step: 4020, train loss: 0.088, val loss: 0.089\n",
      "step: 4030, train loss: 0.087, val loss: 0.091\n",
      "step: 4040, train loss: 0.088, val loss: 0.088\n",
      "step: 4050, train loss: 0.086, val loss: 0.088\n",
      "step: 4060, train loss: 0.087, val loss: 0.085\n",
      "step: 4070, train loss: 0.087, val loss: 0.087\n",
      "step: 4080, train loss: 0.086, val loss: 0.086\n",
      "step: 4090, train loss: 0.086, val loss: 0.088\n",
      "step: 4100, train loss: 0.088, val loss: 0.089\n",
      "step: 4110, train loss: 0.088, val loss: 0.089\n",
      "step: 4120, train loss: 0.088, val loss: 0.086\n",
      "step: 4130, train loss: 0.086, val loss: 0.087\n",
      "step: 4140, train loss: 0.086, val loss: 0.087\n",
      "step: 4150, train loss: 0.088, val loss: 0.087\n",
      "step: 4160, train loss: 0.087, val loss: 0.086\n",
      "step: 4170, train loss: 0.087, val loss: 0.087\n",
      "step: 4180, train loss: 0.086, val loss: 0.087\n",
      "step: 4190, train loss: 0.087, val loss: 0.086\n",
      "step: 4200, train loss: 0.086, val loss: 0.086\n",
      "step: 4210, train loss: 0.088, val loss: 0.086\n",
      "step: 4220, train loss: 0.086, val loss: 0.086\n",
      "step: 4230, train loss: 0.085, val loss: 0.086\n",
      "step: 4240, train loss: 0.087, val loss: 0.087\n",
      "step: 4250, train loss: 0.087, val loss: 0.088\n",
      "step: 4260, train loss: 0.087, val loss: 0.087\n",
      "step: 4270, train loss: 0.083, val loss: 0.086\n",
      "step: 4280, train loss: 0.085, val loss: 0.085\n",
      "step: 4290, train loss: 0.084, val loss: 0.086\n",
      "step: 4300, train loss: 0.085, val loss: 0.085\n",
      "step: 4310, train loss: 0.086, val loss: 0.087\n",
      "step: 4320, train loss: 0.084, val loss: 0.085\n",
      "step: 4330, train loss: 0.085, val loss: 0.086\n",
      "step: 4340, train loss: 0.085, val loss: 0.085\n",
      "step: 4350, train loss: 0.087, val loss: 0.085\n",
      "step: 4360, train loss: 0.084, val loss: 0.084\n",
      "step: 4370, train loss: 0.085, val loss: 0.083\n",
      "step: 4380, train loss: 0.084, val loss: 0.086\n",
      "step: 4390, train loss: 0.085, val loss: 0.085\n",
      "step: 4400, train loss: 0.083, val loss: 0.083\n",
      "step: 4410, train loss: 0.084, val loss: 0.085\n",
      "step: 4420, train loss: 0.085, val loss: 0.085\n",
      "step: 4430, train loss: 0.084, val loss: 0.083\n",
      "step: 4440, train loss: 0.085, val loss: 0.085\n",
      "step: 4450, train loss: 0.086, val loss: 0.085\n",
      "step: 4460, train loss: 0.083, val loss: 0.085\n",
      "step: 4470, train loss: 0.083, val loss: 0.084\n",
      "step: 4480, train loss: 0.084, val loss: 0.085\n",
      "step: 4490, train loss: 0.083, val loss: 0.085\n",
      "step: 4500, train loss: 0.083, val loss: 0.084\n",
      "step: 4510, train loss: 0.083, val loss: 0.084\n",
      "step: 4520, train loss: 0.084, val loss: 0.086\n",
      "step: 4530, train loss: 0.082, val loss: 0.084\n",
      "step: 4540, train loss: 0.082, val loss: 0.085\n",
      "step: 4550, train loss: 0.083, val loss: 0.084\n",
      "step: 4560, train loss: 0.083, val loss: 0.085\n",
      "step: 4570, train loss: 0.082, val loss: 0.084\n",
      "step: 4580, train loss: 0.082, val loss: 0.083\n",
      "step: 4590, train loss: 0.084, val loss: 0.083\n",
      "step: 4600, train loss: 0.084, val loss: 0.083\n",
      "step: 4610, train loss: 0.081, val loss: 0.083\n",
      "step: 4620, train loss: 0.082, val loss: 0.083\n",
      "step: 4630, train loss: 0.082, val loss: 0.082\n",
      "step: 4640, train loss: 0.084, val loss: 0.082\n",
      "step: 4650, train loss: 0.083, val loss: 0.085\n",
      "step: 4660, train loss: 0.083, val loss: 0.083\n",
      "step: 4670, train loss: 0.082, val loss: 0.083\n",
      "step: 4680, train loss: 0.081, val loss: 0.084\n",
      "step: 4690, train loss: 0.081, val loss: 0.081\n",
      "step: 4700, train loss: 0.081, val loss: 0.080\n",
      "step: 4710, train loss: 0.081, val loss: 0.084\n",
      "step: 4720, train loss: 0.081, val loss: 0.080\n",
      "step: 4730, train loss: 0.084, val loss: 0.083\n",
      "step: 4740, train loss: 0.081, val loss: 0.081\n",
      "step: 4750, train loss: 0.082, val loss: 0.081\n",
      "step: 4760, train loss: 0.081, val loss: 0.082\n",
      "step: 4770, train loss: 0.080, val loss: 0.082\n",
      "step: 4780, train loss: 0.083, val loss: 0.080\n",
      "step: 4790, train loss: 0.081, val loss: 0.082\n",
      "step: 4800, train loss: 0.081, val loss: 0.082\n",
      "step: 4810, train loss: 0.081, val loss: 0.082\n",
      "step: 4820, train loss: 0.082, val loss: 0.083\n",
      "step: 4830, train loss: 0.080, val loss: 0.084\n",
      "step: 4840, train loss: 0.080, val loss: 0.081\n",
      "step: 4850, train loss: 0.083, val loss: 0.082\n",
      "step: 4860, train loss: 0.080, val loss: 0.080\n",
      "step: 4870, train loss: 0.079, val loss: 0.080\n",
      "step: 4880, train loss: 0.081, val loss: 0.080\n",
      "step: 4890, train loss: 0.081, val loss: 0.083\n",
      "step: 4900, train loss: 0.083, val loss: 0.082\n",
      "step: 4910, train loss: 0.081, val loss: 0.083\n",
      "step: 4920, train loss: 0.082, val loss: 0.082\n",
      "step: 4930, train loss: 0.079, val loss: 0.081\n",
      "step: 4940, train loss: 0.082, val loss: 0.082\n",
      "step: 4950, train loss: 0.079, val loss: 0.081\n",
      "step: 4960, train loss: 0.079, val loss: 0.083\n",
      "step: 4970, train loss: 0.083, val loss: 0.080\n",
      "step: 4980, train loss: 0.080, val loss: 0.082\n",
      "step: 4990, train loss: 0.082, val loss: 0.078\n",
      "step: 5000, train loss: 0.081, val loss: 0.080\n",
      "step: 5010, train loss: 0.080, val loss: 0.083\n",
      "step: 5020, train loss: 0.081, val loss: 0.082\n",
      "step: 5030, train loss: 0.081, val loss: 0.083\n",
      "step: 5040, train loss: 0.080, val loss: 0.080\n",
      "step: 5050, train loss: 0.080, val loss: 0.082\n",
      "step: 5060, train loss: 0.080, val loss: 0.083\n",
      "step: 5070, train loss: 0.081, val loss: 0.081\n",
      "step: 5080, train loss: 0.083, val loss: 0.080\n",
      "step: 5090, train loss: 0.080, val loss: 0.081\n",
      "step: 5100, train loss: 0.079, val loss: 0.083\n",
      "step: 5110, train loss: 0.081, val loss: 0.079\n",
      "step: 5120, train loss: 0.079, val loss: 0.080\n",
      "step: 5130, train loss: 0.081, val loss: 0.080\n",
      "step: 5140, train loss: 0.081, val loss: 0.080\n",
      "step: 5150, train loss: 0.081, val loss: 0.080\n",
      "step: 5160, train loss: 0.081, val loss: 0.081\n",
      "step: 5170, train loss: 0.082, val loss: 0.080\n",
      "step: 5180, train loss: 0.078, val loss: 0.082\n",
      "step: 5190, train loss: 0.079, val loss: 0.080\n",
      "step: 5200, train loss: 0.081, val loss: 0.081\n",
      "step: 5210, train loss: 0.079, val loss: 0.080\n",
      "step: 5220, train loss: 0.077, val loss: 0.079\n",
      "step: 5230, train loss: 0.077, val loss: 0.080\n",
      "step: 5240, train loss: 0.078, val loss: 0.080\n",
      "step: 5250, train loss: 0.081, val loss: 0.081\n",
      "step: 5260, train loss: 0.080, val loss: 0.083\n",
      "step: 5270, train loss: 0.080, val loss: 0.077\n",
      "step: 5280, train loss: 0.078, val loss: 0.079\n",
      "step: 5290, train loss: 0.078, val loss: 0.080\n",
      "step: 5300, train loss: 0.079, val loss: 0.080\n",
      "step: 5310, train loss: 0.079, val loss: 0.077\n",
      "step: 5320, train loss: 0.078, val loss: 0.081\n",
      "step: 5330, train loss: 0.079, val loss: 0.079\n",
      "step: 5340, train loss: 0.079, val loss: 0.082\n",
      "step: 5350, train loss: 0.078, val loss: 0.081\n",
      "step: 5360, train loss: 0.076, val loss: 0.081\n",
      "step: 5370, train loss: 0.078, val loss: 0.079\n",
      "step: 5380, train loss: 0.075, val loss: 0.081\n",
      "step: 5390, train loss: 0.078, val loss: 0.079\n",
      "step: 5400, train loss: 0.078, val loss: 0.081\n",
      "step: 5410, train loss: 0.077, val loss: 0.080\n",
      "step: 5420, train loss: 0.079, val loss: 0.079\n",
      "step: 5430, train loss: 0.079, val loss: 0.079\n",
      "step: 5440, train loss: 0.076, val loss: 0.077\n",
      "step: 5450, train loss: 0.078, val loss: 0.081\n",
      "step: 5460, train loss: 0.077, val loss: 0.081\n",
      "step: 5470, train loss: 0.080, val loss: 0.078\n",
      "step: 5480, train loss: 0.075, val loss: 0.079\n",
      "step: 5490, train loss: 0.079, val loss: 0.078\n",
      "step: 5500, train loss: 0.079, val loss: 0.075\n",
      "step: 5510, train loss: 0.079, val loss: 0.078\n",
      "step: 5520, train loss: 0.078, val loss: 0.078\n",
      "step: 5530, train loss: 0.079, val loss: 0.078\n",
      "step: 5540, train loss: 0.079, val loss: 0.079\n",
      "step: 5550, train loss: 0.079, val loss: 0.079\n",
      "step: 5560, train loss: 0.077, val loss: 0.080\n",
      "step: 5570, train loss: 0.079, val loss: 0.085\n",
      "step: 5580, train loss: 3.165, val loss: 4.708\n",
      "step: 5590, train loss: 3.554, val loss: 3.104\n",
      "step: 5600, train loss: 3.022, val loss: 2.978\n",
      "step: 5610, train loss: 2.956, val loss: 2.944\n",
      "step: 5620, train loss: 2.946, val loss: 2.921\n",
      "step: 5630, train loss: 2.921, val loss: 2.905\n",
      "step: 5640, train loss: 2.896, val loss: 2.876\n",
      "step: 5650, train loss: 2.870, val loss: 2.828\n",
      "step: 5660, train loss: 2.822, val loss: 2.782\n",
      "step: 5670, train loss: 2.757, val loss: 2.709\n",
      "step: 5680, train loss: 2.653, val loss: 2.552\n",
      "step: 5690, train loss: 2.496, val loss: 2.399\n",
      "step: 5700, train loss: 2.347, val loss: 2.256\n",
      "step: 5710, train loss: 2.125, val loss: 1.950\n",
      "step: 5720, train loss: 1.765, val loss: 1.550\n",
      "step: 5730, train loss: 1.308, val loss: 1.074\n",
      "step: 5740, train loss: 0.952, val loss: 0.795\n",
      "step: 5750, train loss: 0.711, val loss: 0.635\n",
      "step: 5760, train loss: 0.584, val loss: 0.532\n",
      "step: 5770, train loss: 0.492, val loss: 0.450\n",
      "step: 5780, train loss: 0.501, val loss: 0.579\n",
      "step: 5790, train loss: 0.940, val loss: 0.805\n",
      "step: 5800, train loss: 0.695, val loss: 0.556\n",
      "step: 5810, train loss: 0.449, val loss: 0.353\n",
      "step: 5820, train loss: 0.325, val loss: 0.279\n",
      "step: 5830, train loss: 0.255, val loss: 0.244\n",
      "step: 5840, train loss: 0.230, val loss: 0.223\n",
      "step: 5850, train loss: 0.224, val loss: 0.221\n",
      "step: 5860, train loss: 0.216, val loss: 0.214\n",
      "step: 5870, train loss: 0.215, val loss: 0.210\n",
      "step: 5880, train loss: 0.208, val loss: 0.201\n",
      "step: 5890, train loss: 0.192, val loss: 0.179\n",
      "step: 5900, train loss: 0.169, val loss: 0.161\n",
      "step: 5910, train loss: 0.158, val loss: 0.152\n",
      "step: 5920, train loss: 0.148, val loss: 0.146\n",
      "step: 5930, train loss: 0.147, val loss: 0.143\n",
      "step: 5940, train loss: 0.143, val loss: 0.142\n",
      "step: 5950, train loss: 0.138, val loss: 0.141\n",
      "step: 5960, train loss: 0.139, val loss: 0.138\n",
      "step: 5970, train loss: 0.132, val loss: 0.132\n",
      "step: 5980, train loss: 0.130, val loss: 0.133\n",
      "step: 5990, train loss: 0.129, val loss: 0.130\n",
      "step: 6000, train loss: 0.126, val loss: 0.131\n",
      "step: 6010, train loss: 0.125, val loss: 0.123\n",
      "step: 6020, train loss: 0.124, val loss: 0.122\n",
      "step: 6030, train loss: 0.119, val loss: 0.124\n",
      "step: 6040, train loss: 0.120, val loss: 0.121\n",
      "step: 6050, train loss: 0.118, val loss: 0.116\n",
      "step: 6060, train loss: 0.116, val loss: 0.113\n",
      "step: 6070, train loss: 0.112, val loss: 0.114\n",
      "step: 6080, train loss: 0.110, val loss: 0.112\n",
      "step: 6090, train loss: 0.108, val loss: 0.107\n",
      "step: 6100, train loss: 0.108, val loss: 0.109\n",
      "step: 6110, train loss: 0.105, val loss: 0.106\n",
      "step: 6120, train loss: 0.104, val loss: 0.104\n",
      "step: 6130, train loss: 0.103, val loss: 0.103\n",
      "step: 6140, train loss: 0.099, val loss: 0.103\n",
      "step: 6150, train loss: 0.101, val loss: 0.101\n",
      "step: 6160, train loss: 0.101, val loss: 0.101\n",
      "step: 6170, train loss: 0.097, val loss: 0.098\n",
      "step: 6180, train loss: 0.099, val loss: 0.097\n",
      "step: 6190, train loss: 0.098, val loss: 0.098\n",
      "step: 6200, train loss: 0.096, val loss: 0.097\n",
      "step: 6210, train loss: 0.096, val loss: 0.096\n",
      "step: 6220, train loss: 0.095, val loss: 0.093\n",
      "step: 6230, train loss: 0.094, val loss: 0.094\n",
      "step: 6240, train loss: 0.093, val loss: 0.093\n",
      "step: 6250, train loss: 0.091, val loss: 0.092\n",
      "step: 6260, train loss: 0.092, val loss: 0.090\n",
      "step: 6270, train loss: 0.090, val loss: 0.091\n",
      "step: 6280, train loss: 0.089, val loss: 0.090\n",
      "step: 6290, train loss: 0.091, val loss: 0.091\n",
      "step: 6300, train loss: 0.089, val loss: 0.089\n",
      "step: 6310, train loss: 0.087, val loss: 0.089\n",
      "step: 6320, train loss: 0.088, val loss: 0.089\n",
      "step: 6330, train loss: 0.089, val loss: 0.087\n",
      "step: 6340, train loss: 0.086, val loss: 0.086\n",
      "step: 6350, train loss: 0.088, val loss: 0.086\n",
      "step: 6360, train loss: 0.089, val loss: 0.088\n",
      "step: 6370, train loss: 0.088, val loss: 0.088\n",
      "step: 6380, train loss: 0.085, val loss: 0.087\n",
      "step: 6390, train loss: 0.089, val loss: 0.084\n",
      "step: 6400, train loss: 0.085, val loss: 0.086\n",
      "step: 6410, train loss: 0.085, val loss: 0.086\n",
      "step: 6420, train loss: 0.087, val loss: 0.086\n",
      "step: 6430, train loss: 0.087, val loss: 0.088\n",
      "step: 6440, train loss: 0.085, val loss: 0.084\n",
      "step: 6450, train loss: 0.083, val loss: 0.085\n",
      "step: 6460, train loss: 0.084, val loss: 0.085\n",
      "step: 6470, train loss: 0.083, val loss: 0.085\n",
      "step: 6480, train loss: 0.086, val loss: 0.084\n",
      "step: 6490, train loss: 0.085, val loss: 0.088\n",
      "step: 6500, train loss: 0.087, val loss: 0.085\n",
      "step: 6510, train loss: 0.084, val loss: 0.084\n",
      "step: 6520, train loss: 0.084, val loss: 0.085\n",
      "step: 6530, train loss: 0.084, val loss: 0.083\n",
      "step: 6540, train loss: 0.083, val loss: 0.084\n",
      "step: 6550, train loss: 0.083, val loss: 0.084\n",
      "step: 6560, train loss: 0.083, val loss: 0.084\n",
      "step: 6570, train loss: 0.082, val loss: 0.084\n",
      "step: 6580, train loss: 0.083, val loss: 0.083\n",
      "step: 6590, train loss: 0.082, val loss: 0.085\n",
      "step: 6600, train loss: 0.084, val loss: 0.082\n",
      "step: 6610, train loss: 0.082, val loss: 0.083\n",
      "step: 6620, train loss: 0.083, val loss: 0.083\n",
      "step: 6630, train loss: 0.083, val loss: 0.079\n",
      "step: 6640, train loss: 0.080, val loss: 0.083\n",
      "step: 6650, train loss: 0.081, val loss: 0.082\n",
      "step: 6660, train loss: 0.081, val loss: 0.079\n",
      "step: 6670, train loss: 0.080, val loss: 0.082\n",
      "step: 6680, train loss: 0.082, val loss: 0.082\n",
      "step: 6690, train loss: 0.083, val loss: 0.081\n",
      "step: 6700, train loss: 0.083, val loss: 0.082\n",
      "step: 6710, train loss: 0.081, val loss: 0.083\n",
      "step: 6720, train loss: 0.081, val loss: 0.080\n",
      "step: 6730, train loss: 0.081, val loss: 0.082\n",
      "step: 6740, train loss: 0.081, val loss: 0.080\n",
      "step: 6750, train loss: 0.080, val loss: 0.081\n",
      "step: 6760, train loss: 0.081, val loss: 0.082\n",
      "step: 6770, train loss: 0.078, val loss: 0.082\n",
      "step: 6780, train loss: 0.080, val loss: 0.080\n",
      "step: 6790, train loss: 0.081, val loss: 0.080\n",
      "step: 6800, train loss: 0.082, val loss: 0.081\n",
      "step: 6810, train loss: 0.080, val loss: 0.080\n",
      "step: 6820, train loss: 0.081, val loss: 0.082\n",
      "step: 6830, train loss: 0.080, val loss: 0.080\n",
      "step: 6840, train loss: 0.081, val loss: 0.081\n",
      "step: 6850, train loss: 0.078, val loss: 0.083\n",
      "step: 6860, train loss: 0.081, val loss: 0.081\n",
      "step: 6870, train loss: 0.080, val loss: 0.081\n",
      "step: 6880, train loss: 0.079, val loss: 0.081\n",
      "step: 6890, train loss: 0.079, val loss: 0.081\n",
      "step: 6900, train loss: 0.081, val loss: 0.079\n",
      "step: 6910, train loss: 0.080, val loss: 0.079\n",
      "step: 6920, train loss: 0.080, val loss: 0.079\n",
      "step: 6930, train loss: 0.081, val loss: 0.080\n",
      "step: 6940, train loss: 0.079, val loss: 0.081\n",
      "step: 6950, train loss: 0.078, val loss: 0.081\n",
      "step: 6960, train loss: 0.080, val loss: 0.081\n",
      "step: 6970, train loss: 0.077, val loss: 0.079\n",
      "step: 6980, train loss: 0.082, val loss: 0.081\n",
      "step: 6990, train loss: 0.079, val loss: 0.082\n",
      "step: 7000, train loss: 0.081, val loss: 0.082\n",
      "step: 7010, train loss: 0.078, val loss: 0.078\n",
      "step: 7020, train loss: 0.080, val loss: 0.081\n",
      "step: 7030, train loss: 0.079, val loss: 0.079\n",
      "step: 7040, train loss: 0.081, val loss: 0.080\n",
      "step: 7050, train loss: 0.080, val loss: 0.080\n",
      "step: 7060, train loss: 0.081, val loss: 0.080\n",
      "step: 7070, train loss: 0.080, val loss: 0.078\n",
      "step: 7080, train loss: 0.076, val loss: 0.082\n",
      "step: 7090, train loss: 0.082, val loss: 0.079\n",
      "step: 7100, train loss: 0.079, val loss: 0.079\n",
      "step: 7110, train loss: 0.077, val loss: 0.079\n",
      "step: 7120, train loss: 0.080, val loss: 0.080\n",
      "step: 7130, train loss: 0.076, val loss: 0.078\n",
      "step: 7140, train loss: 0.079, val loss: 0.078\n",
      "step: 7150, train loss: 0.078, val loss: 0.078\n",
      "step: 7160, train loss: 0.078, val loss: 0.080\n",
      "step: 7170, train loss: 0.078, val loss: 0.080\n",
      "step: 7180, train loss: 0.076, val loss: 0.079\n",
      "step: 7190, train loss: 0.077, val loss: 0.080\n",
      "step: 7200, train loss: 0.075, val loss: 0.078\n",
      "step: 7210, train loss: 0.078, val loss: 0.077\n",
      "step: 7220, train loss: 0.079, val loss: 0.079\n",
      "step: 7230, train loss: 0.078, val loss: 0.077\n",
      "step: 7240, train loss: 0.077, val loss: 0.080\n",
      "step: 7250, train loss: 0.079, val loss: 0.079\n",
      "step: 7260, train loss: 0.081, val loss: 0.078\n",
      "step: 7270, train loss: 0.077, val loss: 0.079\n",
      "step: 7280, train loss: 0.078, val loss: 0.082\n",
      "step: 7290, train loss: 0.078, val loss: 0.079\n",
      "step: 7300, train loss: 0.079, val loss: 0.078\n",
      "step: 7310, train loss: 0.079, val loss: 0.080\n",
      "step: 7320, train loss: 0.078, val loss: 0.079\n",
      "step: 7330, train loss: 0.078, val loss: 0.078\n",
      "step: 7340, train loss: 0.078, val loss: 0.077\n",
      "step: 7350, train loss: 0.077, val loss: 0.079\n",
      "step: 7360, train loss: 0.076, val loss: 0.078\n",
      "step: 7370, train loss: 0.079, val loss: 0.078\n",
      "step: 7380, train loss: 0.077, val loss: 0.079\n",
      "step: 7390, train loss: 0.076, val loss: 0.077\n",
      "step: 7400, train loss: 0.076, val loss: 0.078\n",
      "step: 7410, train loss: 0.075, val loss: 0.077\n",
      "step: 7420, train loss: 0.074, val loss: 0.078\n",
      "step: 7430, train loss: 0.079, val loss: 0.078\n",
      "step: 7440, train loss: 0.078, val loss: 0.079\n",
      "step: 7450, train loss: 0.076, val loss: 0.078\n",
      "step: 7460, train loss: 0.076, val loss: 0.079\n",
      "step: 7470, train loss: 0.078, val loss: 0.079\n",
      "step: 7480, train loss: 0.075, val loss: 0.077\n",
      "step: 7490, train loss: 0.076, val loss: 0.080\n",
      "step: 7500, train loss: 0.075, val loss: 0.080\n",
      "step: 7510, train loss: 0.079, val loss: 0.075\n",
      "step: 7520, train loss: 0.078, val loss: 0.078\n",
      "step: 7530, train loss: 0.078, val loss: 0.077\n",
      "step: 7540, train loss: 0.077, val loss: 0.078\n",
      "step: 7550, train loss: 0.077, val loss: 0.075\n",
      "step: 7560, train loss: 0.076, val loss: 0.078\n",
      "step: 7570, train loss: 0.074, val loss: 0.077\n",
      "step: 7580, train loss: 0.074, val loss: 0.077\n",
      "step: 7590, train loss: 0.076, val loss: 0.077\n",
      "step: 7600, train loss: 0.078, val loss: 0.077\n",
      "step: 7610, train loss: 0.077, val loss: 0.076\n",
      "step: 7620, train loss: 0.077, val loss: 0.077\n",
      "step: 7630, train loss: 0.077, val loss: 0.076\n",
      "step: 7640, train loss: 0.077, val loss: 0.078\n",
      "step: 7650, train loss: 0.076, val loss: 0.078\n",
      "step: 7660, train loss: 0.078, val loss: 0.077\n",
      "step: 7670, train loss: 0.075, val loss: 0.078\n",
      "step: 7680, train loss: 0.075, val loss: 0.078\n",
      "step: 7690, train loss: 0.078, val loss: 0.075\n",
      "step: 7700, train loss: 0.077, val loss: 0.078\n",
      "step: 7710, train loss: 0.075, val loss: 0.076\n",
      "step: 7720, train loss: 0.074, val loss: 0.076\n",
      "step: 7730, train loss: 0.073, val loss: 0.075\n",
      "step: 7740, train loss: 0.076, val loss: 0.076\n",
      "step: 7750, train loss: 0.075, val loss: 0.077\n",
      "step: 7760, train loss: 0.077, val loss: 0.077\n",
      "step: 7770, train loss: 0.075, val loss: 0.076\n",
      "step: 7780, train loss: 0.074, val loss: 0.077\n",
      "step: 7790, train loss: 0.073, val loss: 0.079\n",
      "step: 7800, train loss: 0.077, val loss: 0.076\n",
      "step: 7810, train loss: 0.078, val loss: 0.078\n",
      "step: 7820, train loss: 0.075, val loss: 0.077\n",
      "step: 7830, train loss: 0.077, val loss: 0.077\n",
      "step: 7840, train loss: 0.075, val loss: 0.077\n",
      "step: 7850, train loss: 0.076, val loss: 0.076\n",
      "step: 7860, train loss: 0.075, val loss: 0.074\n",
      "step: 7870, train loss: 0.076, val loss: 0.078\n",
      "step: 7880, train loss: 0.075, val loss: 0.079\n",
      "step: 7890, train loss: 0.073, val loss: 0.077\n",
      "step: 7900, train loss: 0.078, val loss: 0.077\n",
      "step: 7910, train loss: 0.074, val loss: 0.077\n",
      "step: 7920, train loss: 0.076, val loss: 0.074\n",
      "step: 7930, train loss: 0.076, val loss: 0.077\n",
      "step: 7940, train loss: 0.075, val loss: 0.076\n",
      "step: 7950, train loss: 0.076, val loss: 0.077\n",
      "step: 7960, train loss: 0.073, val loss: 0.074\n",
      "step: 7970, train loss: 0.075, val loss: 0.075\n",
      "step: 7980, train loss: 0.075, val loss: 0.076\n",
      "step: 7990, train loss: 0.076, val loss: 0.077\n",
      "step: 8000, train loss: 0.076, val loss: 0.076\n",
      "step: 8010, train loss: 0.077, val loss: 0.076\n",
      "step: 8020, train loss: 0.077, val loss: 0.078\n",
      "step: 8030, train loss: 0.077, val loss: 0.078\n",
      "step: 8040, train loss: 0.074, val loss: 0.077\n",
      "step: 8050, train loss: 0.076, val loss: 0.077\n",
      "step: 8060, train loss: 0.074, val loss: 0.077\n",
      "step: 8070, train loss: 0.074, val loss: 0.076\n",
      "step: 8080, train loss: 0.075, val loss: 0.076\n",
      "step: 8090, train loss: 0.076, val loss: 0.076\n",
      "step: 8100, train loss: 0.075, val loss: 0.078\n",
      "step: 8110, train loss: 0.075, val loss: 0.074\n",
      "step: 8120, train loss: 0.073, val loss: 0.078\n",
      "step: 8130, train loss: 0.074, val loss: 0.075\n",
      "step: 8140, train loss: 0.078, val loss: 0.075\n",
      "step: 8150, train loss: 0.077, val loss: 0.075\n",
      "step: 8160, train loss: 0.076, val loss: 0.077\n",
      "step: 8170, train loss: 0.075, val loss: 0.076\n",
      "step: 8180, train loss: 0.073, val loss: 0.076\n",
      "step: 8190, train loss: 0.075, val loss: 0.074\n",
      "step: 8200, train loss: 0.073, val loss: 0.075\n",
      "step: 8210, train loss: 0.075, val loss: 0.074\n",
      "step: 8220, train loss: 0.072, val loss: 0.075\n",
      "step: 8230, train loss: 0.073, val loss: 0.074\n",
      "step: 8240, train loss: 0.075, val loss: 0.075\n",
      "step: 8250, train loss: 0.075, val loss: 0.075\n",
      "step: 8260, train loss: 0.075, val loss: 0.074\n",
      "step: 8270, train loss: 0.074, val loss: 0.074\n",
      "step: 8280, train loss: 0.074, val loss: 0.074\n",
      "step: 8290, train loss: 0.075, val loss: 0.074\n",
      "step: 8300, train loss: 0.072, val loss: 0.073\n",
      "step: 8310, train loss: 0.076, val loss: 0.076\n",
      "step: 8320, train loss: 0.074, val loss: 0.074\n",
      "step: 8330, train loss: 0.077, val loss: 0.075\n",
      "step: 8340, train loss: 0.074, val loss: 0.075\n",
      "step: 8350, train loss: 0.076, val loss: 0.075\n",
      "step: 8360, train loss: 0.075, val loss: 0.074\n",
      "step: 8370, train loss: 0.075, val loss: 0.077\n",
      "step: 8380, train loss: 0.074, val loss: 0.077\n",
      "step: 8390, train loss: 0.075, val loss: 0.075\n",
      "step: 8400, train loss: 0.074, val loss: 0.073\n",
      "step: 8410, train loss: 0.074, val loss: 0.073\n",
      "step: 8420, train loss: 0.075, val loss: 0.076\n",
      "step: 8430, train loss: 0.075, val loss: 0.073\n",
      "step: 8440, train loss: 0.076, val loss: 0.075\n",
      "step: 8450, train loss: 0.074, val loss: 0.076\n",
      "step: 8460, train loss: 0.073, val loss: 0.074\n",
      "step: 8470, train loss: 0.075, val loss: 0.076\n",
      "step: 8480, train loss: 0.075, val loss: 0.075\n",
      "step: 8490, train loss: 0.073, val loss: 0.076\n",
      "step: 8500, train loss: 0.076, val loss: 0.076\n",
      "step: 8510, train loss: 0.075, val loss: 0.072\n",
      "step: 8520, train loss: 0.073, val loss: 0.076\n",
      "step: 8530, train loss: 0.074, val loss: 0.073\n",
      "step: 8540, train loss: 0.073, val loss: 0.074\n",
      "step: 8550, train loss: 0.074, val loss: 0.074\n",
      "step: 8560, train loss: 0.074, val loss: 0.075\n",
      "step: 8570, train loss: 0.074, val loss: 0.073\n",
      "step: 8580, train loss: 0.075, val loss: 0.075\n",
      "step: 8590, train loss: 0.076, val loss: 0.078\n",
      "step: 8600, train loss: 0.076, val loss: 0.073\n",
      "step: 8610, train loss: 0.073, val loss: 0.074\n",
      "step: 8620, train loss: 0.073, val loss: 0.077\n",
      "step: 8630, train loss: 0.070, val loss: 0.073\n",
      "step: 8640, train loss: 0.074, val loss: 0.075\n",
      "step: 8650, train loss: 0.073, val loss: 0.075\n",
      "step: 8660, train loss: 0.076, val loss: 0.074\n",
      "step: 8670, train loss: 0.075, val loss: 0.074\n",
      "step: 8680, train loss: 0.075, val loss: 0.074\n",
      "step: 8690, train loss: 0.073, val loss: 0.072\n",
      "step: 8700, train loss: 0.076, val loss: 0.075\n",
      "step: 8710, train loss: 0.074, val loss: 0.078\n",
      "step: 8720, train loss: 0.074, val loss: 0.075\n",
      "step: 8730, train loss: 0.074, val loss: 0.075\n",
      "step: 8740, train loss: 0.073, val loss: 0.075\n",
      "step: 8750, train loss: 0.075, val loss: 0.077\n",
      "step: 8760, train loss: 0.075, val loss: 0.076\n",
      "step: 8770, train loss: 0.074, val loss: 0.075\n",
      "step: 8780, train loss: 0.072, val loss: 0.074\n",
      "step: 8790, train loss: 0.073, val loss: 0.074\n",
      "step: 8800, train loss: 0.073, val loss: 0.076\n",
      "step: 8810, train loss: 0.077, val loss: 0.075\n",
      "step: 8820, train loss: 0.072, val loss: 0.076\n",
      "step: 8830, train loss: 0.074, val loss: 0.075\n",
      "step: 8840, train loss: 0.073, val loss: 0.072\n",
      "step: 8850, train loss: 0.073, val loss: 0.075\n",
      "step: 8860, train loss: 0.073, val loss: 0.074\n",
      "step: 8870, train loss: 0.072, val loss: 0.074\n",
      "step: 8880, train loss: 0.072, val loss: 0.074\n",
      "step: 8890, train loss: 0.073, val loss: 0.075\n",
      "step: 8900, train loss: 0.073, val loss: 0.075\n",
      "step: 8910, train loss: 0.074, val loss: 0.074\n",
      "step: 8920, train loss: 0.073, val loss: 0.076\n",
      "step: 8930, train loss: 0.073, val loss: 0.075\n",
      "step: 8940, train loss: 0.073, val loss: 0.077\n",
      "step: 8950, train loss: 1.605, val loss: 2.604\n",
      "step: 8960, train loss: 2.264, val loss: 1.699\n",
      "step: 8970, train loss: 1.679, val loss: 2.422\n",
      "step: 8980, train loss: 2.129, val loss: 2.069\n",
      "step: 8990, train loss: 1.832, val loss: 1.947\n",
      "step: 9000, train loss: 1.480, val loss: 1.195\n",
      "step: 9010, train loss: 1.139, val loss: 1.070\n",
      "step: 9020, train loss: 1.040, val loss: 1.003\n",
      "step: 9030, train loss: 0.953, val loss: 0.882\n",
      "step: 9040, train loss: 0.818, val loss: 0.746\n",
      "step: 9050, train loss: 0.696, val loss: 0.656\n",
      "step: 9060, train loss: 0.630, val loss: 0.573\n",
      "step: 9070, train loss: 0.469, val loss: 0.292\n",
      "step: 9080, train loss: 0.251, val loss: 0.218\n",
      "step: 9090, train loss: 0.197, val loss: 0.165\n",
      "step: 9100, train loss: 0.149, val loss: 0.139\n",
      "step: 9110, train loss: 0.128, val loss: 0.121\n",
      "step: 9120, train loss: 0.117, val loss: 0.108\n",
      "step: 9130, train loss: 0.105, val loss: 0.101\n",
      "step: 9140, train loss: 0.100, val loss: 0.094\n",
      "step: 9150, train loss: 0.092, val loss: 0.094\n",
      "step: 9160, train loss: 0.090, val loss: 0.090\n",
      "step: 9170, train loss: 0.088, val loss: 0.087\n",
      "step: 9180, train loss: 0.089, val loss: 0.086\n",
      "step: 9190, train loss: 0.084, val loss: 0.085\n",
      "step: 9200, train loss: 0.082, val loss: 0.084\n",
      "step: 9210, train loss: 0.082, val loss: 0.083\n",
      "step: 9220, train loss: 0.080, val loss: 0.081\n",
      "step: 9230, train loss: 0.081, val loss: 0.080\n",
      "step: 9240, train loss: 0.081, val loss: 0.080\n",
      "step: 9250, train loss: 0.079, val loss: 0.080\n",
      "step: 9260, train loss: 0.078, val loss: 0.081\n",
      "step: 9270, train loss: 0.081, val loss: 0.079\n",
      "step: 9280, train loss: 0.077, val loss: 0.080\n",
      "step: 9290, train loss: 0.080, val loss: 0.079\n",
      "step: 9300, train loss: 0.081, val loss: 0.078\n",
      "step: 9310, train loss: 0.080, val loss: 0.078\n",
      "step: 9320, train loss: 0.079, val loss: 0.077\n",
      "step: 9330, train loss: 0.078, val loss: 0.077\n",
      "step: 9340, train loss: 0.077, val loss: 0.078\n",
      "step: 9350, train loss: 0.077, val loss: 0.078\n",
      "step: 9360, train loss: 0.078, val loss: 0.080\n",
      "step: 9370, train loss: 0.076, val loss: 0.078\n",
      "step: 9380, train loss: 0.077, val loss: 0.077\n",
      "step: 9390, train loss: 0.077, val loss: 0.077\n",
      "step: 9400, train loss: 0.077, val loss: 0.077\n",
      "step: 9410, train loss: 0.076, val loss: 0.077\n",
      "step: 9420, train loss: 0.075, val loss: 0.077\n",
      "step: 9430, train loss: 0.075, val loss: 0.077\n",
      "step: 9440, train loss: 0.076, val loss: 0.077\n",
      "step: 9450, train loss: 0.077, val loss: 0.074\n",
      "step: 9460, train loss: 0.078, val loss: 0.076\n",
      "step: 9470, train loss: 0.076, val loss: 0.076\n",
      "step: 9480, train loss: 0.074, val loss: 0.078\n",
      "step: 9490, train loss: 0.074, val loss: 0.075\n",
      "step: 9500, train loss: 0.076, val loss: 0.075\n",
      "step: 9510, train loss: 0.077, val loss: 0.075\n",
      "step: 9520, train loss: 0.074, val loss: 0.077\n",
      "step: 9530, train loss: 0.077, val loss: 0.076\n",
      "step: 9540, train loss: 0.076, val loss: 0.075\n",
      "step: 9550, train loss: 0.077, val loss: 0.075\n",
      "step: 9560, train loss: 0.074, val loss: 0.074\n",
      "step: 9570, train loss: 0.076, val loss: 0.078\n",
      "step: 9580, train loss: 0.073, val loss: 0.074\n",
      "step: 9590, train loss: 0.076, val loss: 0.075\n",
      "step: 9600, train loss: 0.074, val loss: 0.075\n",
      "step: 9610, train loss: 0.074, val loss: 0.077\n",
      "step: 9620, train loss: 0.074, val loss: 0.076\n",
      "step: 9630, train loss: 0.075, val loss: 0.074\n",
      "step: 9640, train loss: 0.076, val loss: 0.074\n",
      "step: 9650, train loss: 0.076, val loss: 0.074\n",
      "step: 9660, train loss: 0.076, val loss: 0.074\n",
      "step: 9670, train loss: 0.073, val loss: 0.076\n",
      "step: 9680, train loss: 0.074, val loss: 0.077\n",
      "step: 9690, train loss: 0.075, val loss: 0.074\n",
      "step: 9700, train loss: 0.074, val loss: 0.077\n",
      "step: 9710, train loss: 0.075, val loss: 0.075\n",
      "step: 9720, train loss: 0.073, val loss: 0.075\n",
      "step: 9730, train loss: 0.074, val loss: 0.077\n",
      "step: 9740, train loss: 0.074, val loss: 0.074\n",
      "step: 9750, train loss: 0.075, val loss: 0.075\n",
      "step: 9760, train loss: 0.074, val loss: 0.075\n",
      "step: 9770, train loss: 0.074, val loss: 0.078\n",
      "step: 9780, train loss: 0.075, val loss: 0.075\n",
      "step: 9790, train loss: 0.072, val loss: 0.073\n",
      "step: 9800, train loss: 0.076, val loss: 0.075\n",
      "step: 9810, train loss: 0.073, val loss: 0.074\n",
      "step: 9820, train loss: 0.071, val loss: 0.074\n",
      "step: 9830, train loss: 0.074, val loss: 0.073\n",
      "step: 9840, train loss: 0.073, val loss: 0.073\n",
      "step: 9850, train loss: 0.075, val loss: 0.076\n",
      "step: 9860, train loss: 0.073, val loss: 0.074\n",
      "step: 9870, train loss: 0.073, val loss: 0.075\n",
      "step: 9880, train loss: 0.074, val loss: 0.074\n",
      "step: 9890, train loss: 0.074, val loss: 0.075\n",
      "step: 9900, train loss: 0.073, val loss: 0.074\n",
      "step: 9910, train loss: 0.074, val loss: 0.074\n",
      "step: 9920, train loss: 0.073, val loss: 0.073\n",
      "step: 9930, train loss: 0.072, val loss: 0.076\n",
      "step: 9940, train loss: 0.072, val loss: 0.075\n",
      "step: 9950, train loss: 0.075, val loss: 0.076\n",
      "step: 9960, train loss: 0.073, val loss: 0.074\n",
      "step: 9970, train loss: 0.072, val loss: 0.074\n",
      "step: 9980, train loss: 0.073, val loss: 0.074\n",
      "step: 9990, train loss: 0.073, val loss: 0.074\n",
      "min loss: 0.06328897627040013\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "steps = 10000\n",
    "log_step = 10\n",
    "for s in range(steps):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    x_enc, x_dec, y = generate_batch(batch_size, block_size, train)\n",
    "    y_pred = model([x_enc, x_dec])\n",
    "    l = loss.calculate(y, y_pred)\n",
    "    history[\"train\"].append(l)\n",
    "\n",
    "    # backward + optimization step\n",
    "    model.backward(loss.backward())\n",
    "    optim.step()\n",
    "\n",
    "    if s % log_step == 0:\n",
    "        x_enc, x_dec, y = generate_batch(batch_size*10, block_size, val)\n",
    "        y_pred = model([x_enc, x_dec])\n",
    "        l = loss.calculate(y, y_pred)\n",
    "        history[\"val\"] += log_step * [l]\n",
    "        print(f\"step: {s}, train loss: {np.mean(history['train'][-log_step:]):.3f}, val loss: {l:.3f}\")\n",
    "\n",
    "print(f\"min loss: {min(history['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[\"train\"], label=\"train\")\n",
    "plt.plot(history[\"val\"], label=\"val\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../figs/loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed tokens:                        | t'st on many a thousand grains t\n",
      "sampling predicted distribution:    | hoh shoor atins, ich the mpeit thatecket dethat hi whiuech wooy slertean: dow than's te! hheem ges y\n",
      "greedy:                             | ou the the the the the the the the the the the the the the the the the the the the the the the the t\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "def sample_multinomial(p):\n",
    "    # source: https://stackoverflow.com/questions/40474436/how-to-apply-numpy-random-choice-to-a-matrix-of-probability-values-vectorized-s\n",
    "    cumsum = p.cumsum(axis=-1)\n",
    "    draws = np.random.rand(*cumsum.shape[:-1], 1)\n",
    "    return (draws < cumsum).argmax(axis=-1)\n",
    "\n",
    "def sample_top(p):\n",
    "    return p.argmax(axis=-1)\n",
    "\n",
    "def generate_tokens(x_seed, out_len, sampling_fcn):\n",
    "    x_enc, x_dec = x_seed\n",
    "    for _ in range(out_len):\n",
    "        y_pred = model([x_enc, x_dec])\n",
    "        y_pred = sampling_fcn(y_pred)[..., -1]\n",
    "        x_dec[..., :-1] = x_dec[..., 1:]\n",
    "        x_dec[..., -1] = y_pred\n",
    "        yield vocab[y_pred]\n",
    "\n",
    "x_enc, x_dec, _ = generate_batch(1, block_size, val)\n",
    "\n",
    "print(\"seed tokens:                        |\", \"\".join([vocab[i] for i in x_dec[0]]))\n",
    "print(\"sampling predicted distribution:    |\", \"\".join([t for t in generate_tokens([x_enc[0], x_dec[0]], 100, sample_multinomial)]))\n",
    "print(\"greedy:                             |\", \"\".join([t for t in generate_tokens([x_enc[0], x_dec[0]], 100, sample_top)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
